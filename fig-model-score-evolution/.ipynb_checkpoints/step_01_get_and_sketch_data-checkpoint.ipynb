{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0318950a",
   "metadata": {},
   "source": [
    "# Assessing the ICON-ModEx ML models\n",
    "\n",
    "## How does the model score change with each ICON-ModEx iteration?\n",
    "This notebook will harvest the data from the `dynamic-learning-rivers` repository to make a plot showing the evolution of the ML models as more traininig data is added.\n",
    "\n",
    "## Important behind the scenes information\n",
    "Here, I am assuming that this notebook is running in a Linux context (e.g. pull a [container that runs a local Jupyter server](https://hub.docker.com/r/jupyter/datascience-notebook) and open the notebook therein). This code will switch branches within a repository that it downloads from GitHub to load the data for side by side comparisions. It will then delete the source repository and retain only the harvested data used for plotting. The harvested data will be saved alongside this notebook here.\n",
    "\n",
    "## Jumping between Python and R\n",
    "Working between these two languages can be done cell-by-cell (i.e. `r2py`) or, in this case, notebook by notebook by launching additional \"subnotebooks\" with `papermill`. We use the latter option here because we want a Python loop to launch a series of R tasks; this is harder to do when all the R must be in a separate cell from the Python.\n",
    "\n",
    "When installing R tidyverse into the conda environment, there are at least three options:\n",
    "1. Direct install from CRAN, e.g.:\n",
    "```\n",
    "install.packages(\"tidyverse\", repos='http://cran.us.r-project.org', quiet=FALSE, Ncpu=4)\n",
    "```\n",
    "2. Conda install from conda-forge:\n",
    "```\n",
    "conda install -c conda-forge r-tidyverse\n",
    "```\n",
    "3. Conda install from anaconda:\n",
    "```\n",
    "conda install -c anaconda r-tidyverse\n",
    "```\n",
    "As of July 2024, there are linking (not compile) errors for some of the dependencies in Option 1. Option 1, since it compiles from source, also takes a relatively long time (10's of minutes). Option 2 seemed like the best way to go since thre are many more downloads of those packages than Option 3, but there are some bugs in how the figure domain is set up and communicated to Jupyter thus impacting some plotting routines. Therefore, Option 3 is the only one that worked end-to-end. It could be that the exact version of Python and/or Jupyter that I am using conflicts with Option 2. The hashes differ on some, but not all, of the R packages installed in Options 2 and 3 (i.e. comparing them with `conda list | grep r-`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bfa85-63de-4d6d-97c4-928490aa37da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run once at start of notebook if these \n",
    "# packages are not already installed\n",
    "! conda install -y -q pandas numpy matplotlib\n",
    "! conda install -y -q papermill\n",
    "! conda install -y -q scikit-learn\n",
    "\n",
    "# Packages needed for R notebook\n",
    "# Conda install pulls binaries from conda-forge, much faster than\n",
    "# installing direct from CRAN (and building from source).\n",
    "# ALso, some packages have linking errors when building \n",
    "# from source.\n",
    "\n",
    "# Install packages from Conda Forge (Option 2, above)\n",
    "#! conda install -y -q -c conda-forge r-tidyverse\n",
    "#! conda install -y -q -c conda-forge r-colourpicker\n",
    "#! conda install -y -q -c conda-forge r-emmeans\n",
    "#! conda install -y -q -c conda-forge r-ggspatial\n",
    "#! conda install -y -q -c conda-forge r-spdata\n",
    "#! conda install -y -q -c conda-forge r-ggpmisc\n",
    "#! conda install -y -q -c conda-forge r-ggextra\n",
    "#! conda install -y -q -c conda-forge r-patchwork\n",
    "\n",
    "# Install packages from Anaconda (Option 3, above)\n",
    "! conda install -y -q -c anaconda r-tidyverse\n",
    "! conda install -y -q -c anaconda r-ggextra\n",
    "! conda install -y -q -c anaconda r-segmented\n",
    "# Convert from .Rmd. Not used\n",
    "#! conda install -y -q jupytext\n",
    "\n",
    "# Get an environment definition file for reproducibility\n",
    "# (i.e. you can rebuild this environment with `conda env update --name <env_name> -f icon-modex-scatter-plot-conda-env.yaml`\n",
    "! conda env export > icon-modex-scatter-plot-conda-env.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import papermill as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# Convert .Rmd subscript to .ipynb\n",
    "#! jupytext --to notebook make_scatter_plot.Rmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae95d7",
   "metadata": {},
   "source": [
    "## Clone source repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc380de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_prefix = '~/tmp/'\n",
    "repo_org='parallelworks'\n",
    "repo_name='dynamic-learning-rivers'\n",
    "repo_url = 'https://github.com/'+repo_org+'/'+repo_name\n",
    "\n",
    "! mkdir -p {repo_prefix}\n",
    "! cd {repo_prefix}; git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3974b3",
   "metadata": {},
   "source": [
    "## Grab data and save to CSV\n",
    "Here we want to track the model score over ModEx iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ce113-6aaf-4815-a6f2-cccb2bfa3846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functionalize all the data preporatory steps based on the\n",
    "# branch name.\n",
    "def prep_data_on_branch(repo_prefix=repo_prefix, repo_name=repo_name, branch_name='main'):\n",
    "    \n",
    "    #===============================\n",
    "    # The following two commands are\n",
    "    # each independent of each other\n",
    "    # in that they run in two separate\n",
    "    # child shells spawned from the\n",
    "    # main underlying execution shell.\n",
    "    # This means that the mkdir command\n",
    "    # is NOT executed within the GitHub\n",
    "    # repository.\n",
    "    #=================================\n",
    "    # Change to the requested branch\n",
    "    ! cd {repo_prefix}/{repo_name}; git checkout {branch_name}\n",
    "    \n",
    "    # Create a working directory for this branch\n",
    "    ! mkdir -p ./intermediate_branch_data/{branch_name}\n",
    "    \n",
    "    # Load data we want to prepare/merge/preprocess\n",
    "    data_prefix=repo_prefix+'/'+repo_name\n",
    "    \n",
    "    # Some of the branches have different amounts of data \n",
    "    # available depending on when they were run and which\n",
    "    # version of the ML model generated the results.\n",
    "    \n",
    "    # Special case of reproducing old community\n",
    "    # meeting results with old ML model\n",
    "    if branch in ['Dec-2021']:\n",
    "        target_name='Respiration_Rate_mg_DO_per_L_per_H'\n",
    "        \n",
    "    # Most likely case of new ML model with updating results\n",
    "    elif branch in [\n",
    "        'June-2023',\n",
    "        'Jul-2023',\n",
    "        'August-2023',\n",
    "        'Sep-2023',\n",
    "        'Oct-2023',\n",
    "        'Nov-2023',\n",
    "        'Dec-2021-log10',\n",
    "        'Sep-2019-log10',\n",
    "        'Jul-2022-log10',\n",
    "        'Aug-2022-log10',\n",
    "        'Sep-2022-log10',\n",
    "        'Oct-2022-log10',\n",
    "        'Nov-2022-log10',\n",
    "        'Dec-2022-log10',\n",
    "        'Jan-2023-log10',\n",
    "        'Feb-2023-log10',\n",
    "        'Mar-2023-log10',\n",
    "        'Apr-2023-log10',\n",
    "        'May-2023-log10',\n",
    "        'June-2023-log10',\n",
    "        'Jul-2023-log10',\n",
    "        'August-2023-log10',\n",
    "        'Sep-2023-log10',\n",
    "        'Oct-2023-log10',\n",
    "        'Nov-2023-log10',\n",
    "        'Nov-2023-log10-DO-update-correct',\n",
    "        'baseline_hp_100',\n",
    "        'baseline_hp_200',\n",
    "        'baseline_hp_300',\n",
    "        'baseline_lp_100',\n",
    "        'baseline_lp_200',\n",
    "        'baseline_lp_300',\n",
    "        'pca_hp_100',\n",
    "        'pca_hp_200',\n",
    "        'pca_hp_300',\n",
    "        'pca_lp_100',\n",
    "        'pca_lp_200',\n",
    "        'pca_lp_300']:\n",
    "        target_name='Normalized_Respiration_Rate_mg_DO_per_H_per_L_sediment'\n",
    "\n",
    "    # Special case of old ML model\n",
    "    else:\n",
    "        target_name='Respiration_Rate_mg_per_L_per_H'\n",
    "        \n",
    "    main_input = pd.read_csv(data_prefix+\"/scripts/prep_01_output_train.csv\")\n",
    "    avg_output = pd.read_csv(data_prefix+'/scripts/post_01_output_ml_predict_avg.csv')\n",
    "    std_output = pd.read_csv(data_prefix+'/scripts/post_01_output_ml_predict_std.csv')\n",
    "    \n",
    "    # Use Sample_ID as the index so files can be merged on this value\n",
    "    main_input.set_index('Sample_ID',inplace=True)\n",
    "    avg_output.set_index('Sample_ID',inplace=True)\n",
    "    std_output.set_index('Sample_ID',inplace=True)\n",
    "\n",
    "    # Merge files\n",
    "    avg_merge = pd.merge(\n",
    "        main_input,\n",
    "        avg_output, \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        suffixes=['_obs','_pre_avg'])\n",
    "    \n",
    "    # Add log calculations for use with R notebook\n",
    "    avg_merge['Log_Observed_Normalized_Respiration_Rate'] = np.log10(np.abs(avg_merge[target_name+'_obs']))\n",
    "    avg_merge['Log_Predicted_Normalized_Respiration_Rate'] = np.log10(np.abs(avg_merge[target_name+'_pre_avg']))\n",
    "    \n",
    "    # Add duplicate columns with uniform target name for\n",
    "    # use later\n",
    "    avg_merge['Observed_Normalized_Respiration_Rate'] = avg_merge[target_name+'_obs']\n",
    "    avg_merge['Predicted_Normalized_Respiration_Rate'] = avg_merge[target_name+'_pre_avg']\n",
    "    \n",
    "    # Add percent error\n",
    "    avg_merge['Observed_RR_percent_error'] = 100.0*np.abs( (\n",
    "        avg_merge['Observed_Normalized_Respiration_Rate'] - avg_merge['Predicted_Normalized_Respiration_Rate']\n",
    "    )/avg_merge['Observed_Normalized_Respiration_Rate'])\n",
    "    \n",
    "    all_merge = pd.merge(\n",
    "        avg_merge,\n",
    "        std_output, \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        suffixes=['','_pre_std'])\n",
    "\n",
    "    predict_and_obs_merge = pd.merge(\n",
    "        avg_output,\n",
    "        std_output,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        suffixes=['_pre_avg','_pre_std'])\n",
    "    \n",
    "    # Write intermediate output files\n",
    "    # Name it the same as what the R notebook expects...\n",
    "    avg_merge.to_csv('intermediate_branch_data/'+branch_name+'/ICON-ModEx_Combined_Predicted_Observed_Respiration_Rates.csv',mode='w')\n",
    "    all_merge.to_csv('intermediate_branch_data/'+branch_name+'/output_obs_avgpre_stdpre_merged.csv',mode='w')\n",
    "    predict_and_obs_merge.to_csv('intermediate_branch_data/'+branch_name+'/output_all_sites_avgpre_stdpre_merged.csv',mode='w')\n",
    "\n",
    "    # Apply filter for points only in CONUS\n",
    "    ! ./conus_filter.sh ./intermediate_branch_data/{branch_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3f6c7-7f2f-4dd9-aa71-a21f750a1807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start from fresh! Delete any .csv files that may already be here\n",
    "!rm -f ICON-ModEx_automated_iterations.csv\n",
    "\n",
    "# Define the threshold for strong/weak respiration rates\n",
    "respiration_threshold = -500.0 \n",
    "\n",
    "# List the branches we want to work with\n",
    "list_branches_scale = [\n",
    "    'Dec-2021',\n",
    "    'Jul-2022',\n",
    "    'Aug-2022',\n",
    "    'Sep-2022',\n",
    "    'Oct-2022',\n",
    "    'Nov-2022',\n",
    "    'Dec-2022',\n",
    "    'Jan-2023',\n",
    "    'Feb-2023',\n",
    "    'Mar-2023',\n",
    "    'ICON-ModEx-v2.1-April-2023-replicate-01',\n",
    "    'ICON-ModEx-v2.1-April-2023-replicate-02',\n",
    "    'May-2023',\n",
    "    'June-2023',\n",
    "    'Jul-2023',\n",
    "    'August-2023',\n",
    "    'Sep-2023',\n",
    "    'Oct-2023',\n",
    "    'Nov-2023']\n",
    "\n",
    "list_branches_log10 = [\n",
    "    'Dec-2021-log10',\n",
    "    'Sep-2019-log10',\n",
    "    'Jul-2022-log10',\n",
    "    'Aug-2022-log10',\n",
    "    'Sep-2022-log10',\n",
    "    'Oct-2022-log10',\n",
    "    'Nov-2022-log10',\n",
    "    'Dec-2022-log10',\n",
    "    'Jan-2023-log10',\n",
    "    'Feb-2023-log10',\n",
    "    'Mar-2023-log10',\n",
    "    'Apr-2023-log10',\n",
    "    'May-2023-log10',\n",
    "    'June-2023-log10',\n",
    "    'Jul-2023-log10',\n",
    "    'August-2023-log10',\n",
    "    'Sep-2023-log10',\n",
    "    'Oct-2023-log10',\n",
    "    'Nov-2023-log10-DO-update-correct']\n",
    "\n",
    "list_branches_baseline = [\n",
    "    'baseline_hp_100',\n",
    "    'baseline_hp_200',\n",
    "    'baseline_hp_300',\n",
    "    'baseline_lp_100',\n",
    "    'baseline_lp_200',\n",
    "    'baseline_lp_300'\n",
    "]\n",
    "\n",
    "list_branches_pca = [\n",
    "    'pca_hp_100',\n",
    "    'pca_hp_200',\n",
    "    'pca_hp_300',\n",
    "    'pca_lp_100',\n",
    "    'pca_lp_200',\n",
    "    'pca_lp_300'\n",
    "]\n",
    "\n",
    "# Decompress CONUS outline dataset\n",
    "! gunzip -c us.xy.gz > us.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd24ea-b722-4bd6-b95b-cd229eb375ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=======================================\n",
    "# Gather data over the standard branches\n",
    "#=======================================\n",
    "\n",
    "# Initialize empty dataframe\n",
    "icon_modex_scale = pd.DataFrame(\n",
    "    columns=['hold-out-avg','hold-out-std','ntrain'])\n",
    "\n",
    "for bb,branch in enumerate(list_branches_scale):\n",
    "    \n",
    "    # Change to the requested branch\n",
    "    # We do not need to prep_data_on_branch for the non-log10 branches\n",
    "    # since we are not plotting the animation of the scatter. But,\n",
    "    # we do need to do this step to generate the ROC-AUC anyway.\n",
    "    prep_data_on_branch(\n",
    "            repo_prefix=repo_prefix, \n",
    "            repo_name=repo_name, \n",
    "            branch_name=branch)\n",
    "    #!cd {repo_prefix}/{repo_name}; git checkout {branch}\n",
    "    \n",
    "    # We want to get the mean and std of the score and plot that vs the number of inputs to train.\n",
    "    # Each dot will be an ICON-ModEx iteration.\n",
    "    \n",
    "    # This file summarizes the holdout score for all the\n",
    "    # Superlearners for each ICON-ModEx iteration.\n",
    "    holdout_score = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/output_data/holdout_score.txt', \n",
    "        sep=':', \n",
    "        names=['Value','Number'], \n",
    "        index_col='Value' )\n",
    "    #holdout_score = pd.read_csv(repo+'/output_data/holdout_score.txt', sep=':')\n",
    "    \n",
    "    # This file lists the total number of data available\n",
    "    # (of which 25% is split to the testing set)\n",
    "    # Some of these points are dropped during training due to\n",
    "    # missing values. A better measure of the actual number of\n",
    "    # data points used to train the models is in the \n",
    "    # alternative file which is present after all the \n",
    "    # preprocessing steps are completed.\n",
    "    # RAW INPUT\n",
    "    #input_data = pd.read_csv(repo+'/input_data/ICON-ModEx_Data.csv')\n",
    "    # PREPROCESSED INPUT\n",
    "    input_data = pd.read_csv(repo_prefix+'/'+repo_name+'/scripts/prep_06_output_final_train.csv')\n",
    "    \n",
    "    # Gather data and force binary classification for ROC-AUC\n",
    "    #tmp_df = pd.read_csv('intermediate_branch_data/'+branch+'/ICON-ModEx_Combined_Predicted_Observed_Respiration_Rates.csv')\n",
    "    #y_true = tmp_df['Observed_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    #y_score = tmp_df['Predicted_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    #roc_auc_score_value = roc_auc_score(y_true,y_score)\n",
    "    #print(str(roc_auc_score_value))\n",
    "    \n",
    "    icon_modex_scale.loc[len(icon_modex_scale.index)] = [\n",
    "        holdout_score.loc['Avg',:].values[0],\n",
    "        holdout_score.loc['Std',:].values[0],\n",
    "        len(input_data.index)\n",
    "        #roc_auc_score_value\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa89c7a-0b05-4735-95aa-cee64f30d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=====================================\n",
    "# Gather data over the -log10 branches\n",
    "#=====================================\n",
    "\n",
    "# Initialize empty dataframe\n",
    "icon_modex_log10 = pd.DataFrame(\n",
    "    columns=['hold-out-avg','hold-out-std','ntrain','roc-auc'])\n",
    "\n",
    "for bb,branch in enumerate(list_branches_log10):\n",
    "    \n",
    "    # Change to the requested branch\n",
    "    prep_data_on_branch(repo_prefix=repo_prefix, repo_name=repo_name, branch_name=branch)\n",
    "    #!cd {repo_prefix}/{repo_name}; git checkout {branch}\n",
    "    \n",
    "    # We want to get the mean and std of the score and plot that vs the number of inputs to train.\n",
    "    # Each dot will be an ICON-ModEx iteration.\n",
    "    \n",
    "    # This file summarizes the holdout score for all the\n",
    "    # Superlearners for each ICON-ModEx iteration.\n",
    "    holdout_score = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/output_data/holdout_score.txt', \n",
    "        sep=':', \n",
    "        names=['Value','Number'], \n",
    "        index_col='Value' )\n",
    "    #holdout_score = pd.read_csv(repo+'/output_data/holdout_score.txt', sep=':')\n",
    "    \n",
    "    # This file lists the total number of data available\n",
    "    # (of which 25% is split to the testing set)\n",
    "    # RAW INPUT (see note above)\n",
    "    #input_data = pd.read_csv(repo+'/input_data/ICON-ModEx_Data.csv')\n",
    "    # PREPROCESSED INPUT\n",
    "    input_data = pd.read_csv(repo_prefix+'/'+repo_name+'/scripts/prep_06_output_final_train.csv')\n",
    "    \n",
    "    # Gather data and force binary classification for ROC-AUC\n",
    "    tmp_df = pd.read_csv('intermediate_branch_data/'+branch+'/ICON-ModEx_Combined_Predicted_Observed_Respiration_Rates.csv')\n",
    "    y_true = tmp_df['Observed_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    y_score = tmp_df['Predicted_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    roc_auc_score_value = roc_auc_score(y_true,y_score)\n",
    "    print(str(roc_auc_score_value))\n",
    "    \n",
    "    icon_modex_log10.loc[len(icon_modex_log10.index)] = [\n",
    "        holdout_score.loc['Avg',:].values[0],\n",
    "        holdout_score.loc['Std',:].values[0],\n",
    "        len(input_data.index),\n",
    "        roc_auc_score_value\n",
    "    ]\n",
    "    \n",
    "    # We could also grab the histogram of CONUS predictions to check for an evolution of bias over time\n",
    "    # Grab code from the Figure-2 harvestor notebook if needed.\n",
    "    # Use Papermill to launch a notebook\n",
    "    # To list available kernels,\n",
    "    #   jupyter kernelspec list\n",
    "    # The working directory was created by prep_data_on_branch\n",
    "    # The first R kernel execution takes a few extra minutes as\n",
    "    # necessary R packages are installed on-the-fly during the \n",
    "    # first execution of the notebook (but subsequent reruns with\n",
    "    # the same kernel do not need to reinstall).\n",
    "    # Wrap this in try: except: because R-plotting raises an error\n",
    "    # after the plot is made (probably a bounding box issue?)\n",
    "    # that we can ignore.\n",
    "    try:\n",
    "        pm.execute_notebook(\n",
    "            'make_scatter_plot.ipynb',\n",
    "            'log_scatter_plot.ipynb',\n",
    "            parameters=dict(work_dir='./intermediate_branch_data/'+branch),\n",
    "            kernel_name='ir'\n",
    "        )\n",
    "    except:\n",
    "        print('Caught error in notebook. Keep going...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31d1af-b58e-4308-b1d6-00461e3d9fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=====================================\n",
    "# Gather data over the baseline branches\n",
    "#=====================================\n",
    "\n",
    "# Initialize empty dataframe\n",
    "icon_modex_baseline = pd.DataFrame(\n",
    "    columns=[\n",
    "        'hold-out-avg','hold-out-std','ntrain','roc-auc',\n",
    "        'r2_test_avg','r2_test_std','r2_test_ens','hp_or_lp_flag'])\n",
    "\n",
    "# Same target for all baseline branches\n",
    "target_name = 'Normalized_Respiration_Rate_mg_DO_per_H_per_L_sediment'\n",
    "\n",
    "for bb,branch in enumerate(list_branches_baseline):\n",
    "    \n",
    "    # Change to the requested branch\n",
    "    prep_data_on_branch(repo_prefix=repo_prefix, repo_name=repo_name, branch_name=branch)\n",
    "    #!cd {repo_prefix}/{repo_name}; git checkout {branch}\n",
    "    \n",
    "    # This file summarizes the holdout score for all the\n",
    "    # Superlearners for each ICON-ModEx iteration.\n",
    "    holdout_score = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/output_data/holdout_score.txt', \n",
    "        sep=':', \n",
    "        names=['Value','Number'], \n",
    "        index_col='Value' )\n",
    "    #holdout_score = pd.read_csv(repo+'/output_data/holdout_score.txt', sep=':')\n",
    "    \n",
    "    # This file lists the total number of data available\n",
    "    # (of which 25% is split to the testing set)\n",
    "    # RAW INPUT (see note above)\n",
    "    #input_data = pd.read_csv(repo+'/input_data/ICON-ModEx_Data.csv')\n",
    "    # PREPROCESSED INPUT\n",
    "    input_data = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/scripts/prep_06_output_final_train.csv')\n",
    "    \n",
    "    # Gather data and force binary classification for ROC-AUC\n",
    "    tmp_df = pd.read_csv('intermediate_branch_data/'+branch+'/ICON-ModEx_Combined_Predicted_Observed_Respiration_Rates.csv')\n",
    "    y_true = tmp_df['Observed_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    y_score = tmp_df['Predicted_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    roc_auc_score_value = roc_auc_score(y_true,y_score)\n",
    "    #print(str(roc_auc_score_value))\n",
    "    \n",
    "    #---------------------------------------------------------\n",
    "    # Also we want to do something a little different with the\n",
    "    # baseline branches - they are evaluated on a truly\n",
    "    # independent testing set (and not through the Monte\n",
    "    # Carlo cross-validation that effectively uses all points)\n",
    "    # So to create a comparable R2, we need to:\n",
    "    #---------------------------------------------------------\n",
    "    # 1) Get the respiration rates of the 84 sites in the testing set\n",
    "    #---------------------------------------------------------\n",
    "    # Keep it in the DF for now because we want the respiration\n",
    "    # rates linked to their site ID.\n",
    "    test_df = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/examples/ModEx_reproducibility/hp_lp_training_data_based_on_combined_metric/mp_084.csv.out')\n",
    "    \n",
    "    # Use Sample_ID as the index so files can be merged on this value\n",
    "    test_df.set_index('Sample_ID',inplace=True)\n",
    "    \n",
    "    #---------------------------------------------------------\n",
    "    # 2) Loop over each iteration...\n",
    "    #---------------------------------------------------------\n",
    "    r2_score_list = []\n",
    "    for ll in np.arange(0,10):\n",
    "        # Extract the predictions at the 84 sites we're using as a baseline\n",
    "        predict_df = pd.read_csv(repo_prefix+'/'+repo_name+'/ml_models/sl_'+str(ll)+'/sl_predictions.csv')\n",
    "        \n",
    "        # Use Sample_ID as the index so files can be merged on this value\n",
    "        predict_df.set_index('Sample_ID',inplace=True)\n",
    "        \n",
    "        # Merge the rows on this predict_df with the rows of the test_df,\n",
    "        # keeping only the rows that match the test_df. Columns with the\n",
    "        # same names (in particular the target) will get appended _obs\n",
    "        # and _pre as in prep_data_on_branch.\n",
    "        merge_df = pd.merge(\n",
    "            test_df,\n",
    "            predict_df, \n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=['_obs','_pre'])\n",
    "        \n",
    "        # Output dataframe for this ensemble member to check shapes, etc.\n",
    "        merge_df.to_csv(\n",
    "            './intermediate_branch_data/'+branch+'/sl_'+str(ll)+'_obs_predict_merge.csv', mode='w')\n",
    "        \n",
    "        # Compute the r2_score for that iteration for those 84 sites\n",
    "        my_r = np.min(np.corrcoef(\n",
    "            merge_df[target_name+'_obs'],\n",
    "            merge_df[target_name+'_pre']\n",
    "        ))\n",
    "        \n",
    "        my_r2 = my_r*my_r\n",
    "        \n",
    "        # If r2_score is less than 0, set to 0. The magnitude of\n",
    "        # values less than 0 don't mean anything; they are all\n",
    "        # just really bad ML models.\n",
    "        if my_r2 < 0.0:\n",
    "            my_r2 = 0.0\n",
    "        \n",
    "        # Add r2_score to list\n",
    "        r2_score_list.append(my_r2)\n",
    "    \n",
    "    # Find the mean and std of the r2_score over the iterations for the testing set (gray band in plot)\n",
    "    r2_test_avg = np.mean(r2_score_list)\n",
    "    r2_test_std = np.std(r2_score_list)\n",
    "    \n",
    "    # Find the r2_score of the ensemble mean prediction for the testing set (should fall in the middle of the gray band in plot)\n",
    "    predict_df = pd.read_csv(repo_prefix+'/'+repo_name+'/scripts/post_01_output_ml_predict_avg.csv')\n",
    "    predict_df.set_index('Sample_ID',inplace=True)\n",
    "    merge_df = pd.merge(\n",
    "            test_df,\n",
    "            predict_df, \n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=['_obs','_pre'])\n",
    "    merge_df.to_csv(\n",
    "            './intermediate_branch_data/'+branch+'/ens_obs_predict_merge.csv', mode='w')\n",
    "    r_test_ens = np.min(np.corrcoef(\n",
    "        merge_df[target_name+'_obs'],\n",
    "        merge_df[target_name+'_pre']\n",
    "    ))\n",
    "    r2_test_ens = r_test_ens*r_test_ens\n",
    "    if r2_test_ens < 0:\n",
    "        r2_test_ens = 0.0\n",
    "    \n",
    "    # Set a flag to know whether this is an HP or LP model\n",
    "    # (for plotting later). Value for LP is 0, HP is 1.\n",
    "    hp_or_lp_flag = 0\n",
    "    if ( branch.__contains__('hp') ):\n",
    "        hp_or_lp_flag = 1\n",
    "    \n",
    "    # Write output\n",
    "    # Dataframe used here was initialized with:\n",
    "    # columns=['hold-out-avg','hold-out-std','ntrain','roc-auc',\n",
    "    #          'r2_test_avg','r2_test_std','r2_test_ens','hp_or_lp_flag']\n",
    "    icon_modex_baseline.loc[len(icon_modex_baseline.index)] = [\n",
    "        holdout_score.loc['Avg',:].values[0],\n",
    "        holdout_score.loc['Std',:].values[0],\n",
    "        len(input_data.index),\n",
    "        roc_auc_score_value,\n",
    "        r2_test_avg,\n",
    "        r2_test_std,\n",
    "        r2_test_ens,\n",
    "        hp_or_lp_flag\n",
    "    ]\n",
    "    \n",
    "# Compare to the hold-out score - is the hold-out at all a reasonable representation of R2 computed with a fully independent data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305eb325-f325-44b5-ba95-bc4de69645a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=====================================\n",
    "# Gather data over the pca branches\n",
    "#=====================================\n",
    "\n",
    "# Initialize empty dataframe\n",
    "icon_modex_pca = pd.DataFrame(\n",
    "    columns=[\n",
    "        'hold-out-avg','hold-out-std','ntrain','roc-auc',\n",
    "        'r2_test_avg','r2_test_std','r2_test_ens','hp_or_lp_flag'])\n",
    "\n",
    "# Same target for all baseline branches\n",
    "target_name = 'Normalized_Respiration_Rate_mg_DO_per_H_per_L_sediment'\n",
    "\n",
    "for bb,branch in enumerate(list_branches_pca):\n",
    "    \n",
    "    # Change to the requested branch\n",
    "    prep_data_on_branch(repo_prefix=repo_prefix, repo_name=repo_name, branch_name=branch)\n",
    "    #!cd {repo_prefix}/{repo_name}; git checkout {branch}\n",
    "    \n",
    "    # This file summarizes the holdout score for all the\n",
    "    # Superlearners for each ICON-ModEx iteration.\n",
    "    holdout_score = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/output_data/holdout_score.txt', \n",
    "        sep=':', \n",
    "        names=['Value','Number'], \n",
    "        index_col='Value' )\n",
    "    #holdout_score = pd.read_csv(repo+'/output_data/holdout_score.txt', sep=':')\n",
    "    \n",
    "    # This file lists the total number of data available\n",
    "    # (of which 25% is split to the testing set)\n",
    "    # RAW INPUT (see note above)\n",
    "    #input_data = pd.read_csv(repo+'/input_data/ICON-ModEx_Data.csv')\n",
    "    # PREPROCESSED INPUT\n",
    "    input_data = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/scripts/prep_06_output_final_train.csv')\n",
    "    \n",
    "    # Gather data and force binary classification for ROC-AUC\n",
    "    tmp_df = pd.read_csv('intermediate_branch_data/'+branch+'/ICON-ModEx_Combined_Predicted_Observed_Respiration_Rates.csv')\n",
    "    y_true = tmp_df['Observed_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    y_score = tmp_df['Predicted_Normalized_Respiration_Rate'] > respiration_threshold\n",
    "    roc_auc_score_value = roc_auc_score(y_true,y_score)\n",
    "    #print(str(roc_auc_score_value))\n",
    "    \n",
    "    #---------------------------------------------------------\n",
    "    # Also we want to do something a little different with the\n",
    "    # baseline branches - they are evaluated on a truly\n",
    "    # independent testing set (and not through the Monte\n",
    "    # Carlo cross-validation that effectively uses all points)\n",
    "    # So to create a comparable R2, we need to:\n",
    "    #---------------------------------------------------------\n",
    "    # 1) Get the respiration rates of the 84 sites in the testing set\n",
    "    #---------------------------------------------------------\n",
    "    # Keep it in the DF for now because we want the respiration\n",
    "    # rates linked to their site ID.\n",
    "    test_df = pd.read_csv(\n",
    "        repo_prefix+'/'+repo_name+'/examples/ModEx_reproducibility/hp_lp_training_data_based_on_pca_dist/mp_084.csv.out')\n",
    "    \n",
    "    # Use Sample_ID as the index so files can be merged on this value\n",
    "    test_df.set_index('Sample_ID',inplace=True)\n",
    "    \n",
    "    #---------------------------------------------------------\n",
    "    # 2) Loop over each iteration...\n",
    "    #---------------------------------------------------------\n",
    "    r2_score_list = []\n",
    "    for ll in np.arange(0,10):\n",
    "        # Extract the predictions at the 84 sites we're using as a baseline\n",
    "        predict_df = pd.read_csv(repo_prefix+'/'+repo_name+'/ml_models/sl_'+str(ll)+'/sl_predictions.csv')\n",
    "        \n",
    "        # Use Sample_ID as the index so files can be merged on this value\n",
    "        predict_df.set_index('Sample_ID',inplace=True)\n",
    "        \n",
    "        # Merge the rows on this predict_df with the rows of the test_df,\n",
    "        # keeping only the rows that match the test_df. Columns with the\n",
    "        # same names (in particular the target) will get appended _obs\n",
    "        # and _pre as in prep_data_on_branch.\n",
    "        merge_df = pd.merge(\n",
    "            test_df,\n",
    "            predict_df, \n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=['_obs','_pre'])\n",
    "        \n",
    "        # Output dataframe for this ensemble member to check shapes, etc.\n",
    "        merge_df.to_csv(\n",
    "            './intermediate_branch_data/'+branch+'/sl_'+str(ll)+'_obs_predict_merge.csv', mode='w')\n",
    "        \n",
    "        # Compute the r2_score for that iteration for those 84 sites\n",
    "        my_r = np.min(np.corrcoef(\n",
    "            merge_df[target_name+'_obs'],\n",
    "            merge_df[target_name+'_pre']\n",
    "        ))\n",
    "        \n",
    "        my_r2 = my_r*my_r\n",
    "        \n",
    "        # If r2_score is less than 0, set to 0. The magnitude of\n",
    "        # values less than 0 don't mean anything; they are all\n",
    "        # just really bad ML models.\n",
    "        if my_r2 < 0.0:\n",
    "            my_r2 = 0.0\n",
    "        \n",
    "        # Add r2_score to list\n",
    "        r2_score_list.append(my_r2)\n",
    "    \n",
    "    # Find the mean and std of the r2_score over the iterations for the testing set (gray band in plot)\n",
    "    r2_test_avg = np.mean(r2_score_list)\n",
    "    r2_test_std = np.std(r2_score_list)\n",
    "    \n",
    "    # Find the r2_score of the ensemble mean prediction for the testing set (should fall in the middle of the gray band in plot)\n",
    "    predict_df = pd.read_csv(repo_prefix+'/'+repo_name+'/scripts/post_01_output_ml_predict_avg.csv')\n",
    "    predict_df.set_index('Sample_ID',inplace=True)\n",
    "    merge_df = pd.merge(\n",
    "            test_df,\n",
    "            predict_df, \n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=['_obs','_pre'])\n",
    "    merge_df.to_csv(\n",
    "            './intermediate_branch_data/'+branch+'/ens_obs_predict_merge.csv', mode='w')\n",
    "    r_test_ens = np.min(np.corrcoef(\n",
    "        merge_df[target_name+'_obs'],\n",
    "        merge_df[target_name+'_pre']\n",
    "    ))\n",
    "    r2_test_ens = r_test_ens*r_test_ens\n",
    "    if r2_test_ens < 0:\n",
    "        r2_test_ens = 0.0\n",
    "    \n",
    "    # Set a flag to know whether this is an HP or LP model\n",
    "    # (for plotting later). Value for LP is 0, HP is 1.\n",
    "    hp_or_lp_flag = 0\n",
    "    if ( branch.__contains__('hp') ):\n",
    "        hp_or_lp_flag = 1\n",
    "    \n",
    "    # Write output\n",
    "    # Dataframe used here was initialized with:\n",
    "    # columns=['hold-out-avg','hold-out-std','ntrain','roc-auc',\n",
    "    #          'r2_test_avg','r2_test_std','r2_test_ens','hp_or_lp_flag']\n",
    "    icon_modex_pca.loc[len(icon_modex_pca.index)] = [\n",
    "        holdout_score.loc['Avg',:].values[0],\n",
    "        holdout_score.loc['Std',:].values[0],\n",
    "        len(input_data.index),\n",
    "        roc_auc_score_value,\n",
    "        r2_test_avg,\n",
    "        r2_test_std,\n",
    "        r2_test_ens,\n",
    "        hp_or_lp_flag\n",
    "    ]\n",
    "    \n",
    "# Compare to the hold-out score - is the hold-out at all a reasonable representation of R2 computed with a fully independent data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1c6f2-888a-46db-a984-c44ccedfd59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example with high resolution images\n",
    "#! convert -size 1080x1080 -delay 2 -loop 0 *.png output.gif\n",
    "\n",
    "# Can't use wildcard this b/c lists files alphabetically (i.e. by month \n",
    "# name, not month order).\n",
    "#! convert -delay 2 -loop 0 intermediate_branch_data/*/*.png scatter.gif\n",
    "\n",
    "# Let's just manually prescribe the order since it's not that many \n",
    "# and there are some special cases. Make a temporary symbolic link\n",
    "# to each file locally and name the links so that they go in the\n",
    "# exact order of the ModEx iterations. Then, create the animation\n",
    "# with convert and delete the symbolic links.\n",
    "for bb,branch in enumerate(list_branches_log10):\n",
    "    cc = bb + 100\n",
    "    ! ln -sv intermediate_branch_data/{branch}/scatter.png ./tmp_{cc}.png\n",
    "    \n",
    "! convert -delay 100 -loop 0 tmp_*.png scatter.gif\n",
    "! rm -f tmp_*.png\n",
    "\n",
    "for bb,branch in enumerate(list_branches_log10):\n",
    "    cc = bb + 100\n",
    "    ! ln -sv intermediate_branch_data/{branch}/scatter_no_log.png ./tmp_{cc}.png\n",
    "    \n",
    "! convert -delay 100 -loop 0 tmp_*.png scatter_no_log.gif\n",
    "! rm -f tmp_*.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679564c",
   "metadata": {},
   "source": [
    "# Load and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d3155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# Set all font sizes in the plots\n",
    "font = {'family' : 'sans',\n",
    "        'style'  : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 30}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "# Plot specs\n",
    "e_bar_w=4\n",
    "e_cap_w=5\n",
    "e_sym_s=12\n",
    "\n",
    "icon_modex_manual = pd.read_csv('ICON-ModEx_manual_iterations.csv')\n",
    "\n",
    "# These iterations are the very first automated iterations\n",
    "# presented at AGU2022. They were done in a \"hindcast mode\"\n",
    "# to be able to compare to the manual iterations.\n",
    "icon_modex_raw = pd.read_csv('ICON-ModEx_automated_iterations_raw.csv')\n",
    "\n",
    "# Create polygons for background\n",
    "xy_hp_test = []\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][0],\n",
    "    icon_modex_baseline['r2_test_avg'][0]+icon_modex_baseline['r2_test_std'][0]])\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][1],\n",
    "    icon_modex_baseline['r2_test_avg'][1]+icon_modex_baseline['r2_test_std'][1]])\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][2],\n",
    "    icon_modex_baseline['r2_test_avg'][2]+icon_modex_baseline['r2_test_std'][2]])\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][2],\n",
    "    icon_modex_baseline['r2_test_avg'][2]-icon_modex_baseline['r2_test_std'][2]])\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][1],\n",
    "    icon_modex_baseline['r2_test_avg'][1]-icon_modex_baseline['r2_test_std'][1]])\n",
    "xy_hp_test.append([\n",
    "    icon_modex_baseline['ntrain'][0],\n",
    "    icon_modex_baseline['r2_test_avg'][0]-icon_modex_baseline['r2_test_std'][0]])\n",
    "\n",
    "xy_lp_test = []\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][3],\n",
    "    icon_modex_baseline['r2_test_avg'][3]+icon_modex_baseline['r2_test_std'][3]])\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][4],\n",
    "    icon_modex_baseline['r2_test_avg'][4]+icon_modex_baseline['r2_test_std'][4]])\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][5],\n",
    "    icon_modex_baseline['r2_test_avg'][5]+icon_modex_baseline['r2_test_std'][5]])\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][5],\n",
    "    icon_modex_baseline['r2_test_avg'][5]-icon_modex_baseline['r2_test_std'][5]])\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][4],\n",
    "    icon_modex_baseline['r2_test_avg'][4]-icon_modex_baseline['r2_test_std'][4]])\n",
    "xy_lp_test.append([\n",
    "    icon_modex_baseline['ntrain'][3],\n",
    "    icon_modex_baseline['r2_test_avg'][3]-icon_modex_baseline['r2_test_std'][3]])\n",
    "\n",
    "xy_pca_hp_test = []\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][0],\n",
    "    icon_modex_pca['r2_test_avg'][0]+icon_modex_pca['r2_test_std'][0]])\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][1],\n",
    "    icon_modex_pca['r2_test_avg'][1]+icon_modex_pca['r2_test_std'][1]])\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][2],\n",
    "    icon_modex_pca['r2_test_avg'][2]+icon_modex_pca['r2_test_std'][2]])\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][2],\n",
    "    icon_modex_pca['r2_test_avg'][2]-icon_modex_pca['r2_test_std'][2]])\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][1],\n",
    "    icon_modex_pca['r2_test_avg'][1]-icon_modex_pca['r2_test_std'][1]])\n",
    "xy_pca_hp_test.append([\n",
    "    icon_modex_pca['ntrain'][0],\n",
    "    icon_modex_pca['r2_test_avg'][0]-icon_modex_pca['r2_test_std'][0]])\n",
    "\n",
    "xy_pca_lp_test = []\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][3],\n",
    "    icon_modex_pca['r2_test_avg'][3]+icon_modex_pca['r2_test_std'][3]])\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][4],\n",
    "    icon_modex_pca['r2_test_avg'][4]+icon_modex_pca['r2_test_std'][4]])\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][5],\n",
    "    icon_modex_pca['r2_test_avg'][5]+icon_modex_pca['r2_test_std'][5]])\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][5],\n",
    "    icon_modex_pca['r2_test_avg'][5]-icon_modex_pca['r2_test_std'][5]])\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][4],\n",
    "    icon_modex_pca['r2_test_avg'][4]-icon_modex_pca['r2_test_std'][4]])\n",
    "xy_pca_lp_test.append([\n",
    "    icon_modex_pca['ntrain'][3],\n",
    "    icon_modex_pca['r2_test_avg'][3]-icon_modex_pca['r2_test_std'][3]])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "# No need for manual iterations since recreated by workflow\n",
    "#ax.errorbar(\n",
    "#    icon_modex_manual['ntrain'],\n",
    "#    icon_modex_manual[['hold-out','cv_avg']].max(axis=1),\n",
    "#    yerr=icon_modex_manual['cv_std'],\n",
    "#    fmt='k+',\n",
    "#    ecolor='lightgray',\n",
    "#    capsize=0)\n",
    "#plt.errorbar(\n",
    "#    icon_modex_raw['ntrain'],\n",
    "#    icon_modex_raw['hold-out-avg'],\n",
    "#    yerr=icon_modex_raw['hold-out-std'],\n",
    "#    fmt='ko-',\n",
    "#    ecolor='r',\n",
    "#    capsize=5)\n",
    "\n",
    "#==========================================\n",
    "# Selectively retain the HP/LP baseline/pca\n",
    "# polygons. These are the model scores computed\n",
    "# with the full hold out data (the 84 MP \"neutral\"\n",
    "# sites that were never used during training)\n",
    "# and as such the model scores are lower. But the\n",
    "# overall story is similar for the baseline case: \n",
    "# HP sites result in higher scores than LP sites \n",
    "# and the difference becomes significant as you \n",
    "# add more sites in the training. There does not\n",
    "# appear to be any significant difference when\n",
    "# using the HP/LP sites ranked by PCA. In any case,\n",
    "# remove these from the plot since it's not a\n",
    "# fair apples-to-apples comparison.\n",
    "if (False):\n",
    "    ax.add_collection(\n",
    "        PatchCollection(\n",
    "            [Polygon(xy_hp_test, closed=True) ],\n",
    "            alpha=0.4,\n",
    "            color='r'))\n",
    "    ax.add_collection(\n",
    "        PatchCollection(\n",
    "            [Polygon(xy_lp_test, closed=True) ],\n",
    "            alpha=0.4,\n",
    "            color='k'))\n",
    "if (False):\n",
    "    ax.add_collection(\n",
    "        PatchCollection(\n",
    "            [Polygon(xy_pca_hp_test, closed=True) ],\n",
    "            alpha=0.4,\n",
    "            color='r'))\n",
    "    ax.add_collection(\n",
    "        PatchCollection(\n",
    "            [Polygon(xy_pca_lp_test, closed=True) ],\n",
    "            alpha=0.4,\n",
    "            color='k'))\n",
    "\n",
    "ax.errorbar(\n",
    "    icon_modex_scale['ntrain'],\n",
    "    icon_modex_scale['hold-out-avg'],\n",
    "    icon_modex_scale['hold-out-std'],\n",
    "    fmt='ko',\n",
    "    ecolor='r',\n",
    "    elinewidth=e_bar_w,\n",
    "    capsize=e_cap_w,\n",
    "    markersize=e_sym_s)\n",
    "ax.errorbar(\n",
    "    icon_modex_log10['ntrain'],\n",
    "    icon_modex_log10['hold-out-avg'],\n",
    "    icon_modex_log10['hold-out-std'],\n",
    "    fmt='ks',\n",
    "    ecolor='b',\n",
    "    elinewidth=e_bar_w,\n",
    "    capsize=e_cap_w,\n",
    "    markersize=e_sym_s)\n",
    "\n",
    "# First all the HP points\n",
    "hp=icon_modex_baseline[icon_modex_baseline['hp_or_lp_flag'] == 1.0]\n",
    "ax.errorbar(\n",
    "    hp['ntrain'],\n",
    "    hp['hold-out-avg'],\n",
    "    hp['hold-out-std'],\n",
    "    fmt='kd',\n",
    "    ecolor='g',\n",
    "    elinewidth=e_bar_w,\n",
    "    capsize=e_cap_w,\n",
    "    markersize=e_sym_s)\n",
    "\n",
    "# Second all the LP points\n",
    "hp=icon_modex_baseline[icon_modex_baseline['hp_or_lp_flag'] == 0.0]\n",
    "ax.errorbar(\n",
    "    hp['ntrain'],\n",
    "    hp['hold-out-avg'],\n",
    "    hp['hold-out-std'],\n",
    "    fmt='kv',\n",
    "    ecolor='y',\n",
    "    elinewidth=e_bar_w,\n",
    "    capsize=e_cap_w,\n",
    "    markersize=e_sym_s)\n",
    "\n",
    "# Comment out the HP/LP PCA points since they\n",
    "# tell a similar, but less robust, story.\n",
    "#ax.errorbar(\n",
    "#    # Artificial offset for visualization\n",
    "#    icon_modex_pca['ntrain']+10,\n",
    "#    icon_modex_pca['hold-out-avg'],\n",
    "#    icon_modex_pca['hold-out-std'],\n",
    "#    fmt='ys',\n",
    "#    ecolor='y',\n",
    "#    elinewidth=3,\n",
    "#    capsize=5)\n",
    "\n",
    "#ax.errorbar(\n",
    "#    icon_modex_baseline['ntrain'],\n",
    "#    icon_modex_baseline['r2_test_avg'],\n",
    "#    icon_modex_baseline['r2_test_std'],\n",
    "#    fmt='cs',\n",
    "#    ecolor='c',\n",
    "#    elinewidth=3,\n",
    "#    capsize=5)\n",
    "ax.grid()\n",
    "\n",
    "plt.ylabel('Avg ML Model Score of Training data set ($R^2$)')\n",
    "plt.xlabel('Total number of samples in input data set ($N$)')\n",
    "plt.legend(['ModEx live','ModEx rerun','HP baseline','LP baseline'],loc='lower right')\n",
    "plt.savefig('ICON_ModEx_summary.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6b41f",
   "metadata": {},
   "source": [
    "**Figure Caption:** The \"live\" iterations are the ICON-ModEx ML models that generated the results shared with community members and were used for making decisions of which sites to sample. The hindcast iterations use both the respiration rate normalized with respect to sediment volume and a log10 transform on the targets because both approaches together minimized the bias of the ML models while also increasing the model scores. The big jump in the live iterations is due to the adoption of the normalization of the respiration rate data with respect to the sediment volume and applying the log10 transform in the \"production run\" of June of 2023 and all subsequent ML model iterations. The hindcast runs all take advantage of the normalization and the log10 transform to provide an apples-to-apples comparision of the progress of the ICON-ModEx iterations had these changes been applied since the very beginning of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d5c5a-fbcb-4436-8802-5d4206c35b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icon_modex_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55022461-186e-42a8-a83d-4f756c54b4b1",
   "metadata": {},
   "source": [
    "## Plot the ROC-AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabcc6a-1bf9-42cb-b2cd-6958f4d09a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "# Did not save this data in old archived ML model results\n",
    "#ax.plot(\n",
    "#    icon_modex_scale['ntrain'],\n",
    "#    icon_modex_scale['roc-auc'],'r.')\n",
    "ax.plot(\n",
    "    icon_modex_log10['ntrain'],\n",
    "    icon_modex_log10['roc-auc'],'b.',markersize=10)\n",
    "ax.grid()\n",
    "\n",
    "plt.ylabel('ROC AUC score for each ICON-ModEx iteration')\n",
    "plt.xlabel('Total number of samples in input data set')\n",
    "plt.legend(['ModEx - log10 hindcast'])\n",
    "plt.savefig('ICON_ModEx_ROC_AUC.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161c6fc-ca91-46d5-ad67-34f9fe8fbdab",
   "metadata": {},
   "source": [
    "# Load and plot simplified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162917f8-a430-4b13-a79c-bb8ecf3c4c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icon_modex_manual = pd.read_csv('ICON-ModEx_manual_iterations.csv')\n",
    "\n",
    "# These iterations are the very first automated iterations\n",
    "# presented at AGU2022. They were done in a \"hindcast mode\"\n",
    "# to be able to compare to the manual iterations.\n",
    "icon_modex_raw = pd.read_csv('ICON-ModEx_automated_iterations_raw.csv')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "plt.errorbar(\n",
    "    icon_modex_manual['ntrain'],\n",
    "    icon_modex_manual[['hold-out','cv_avg']].max(axis=1),\n",
    "    yerr=icon_modex_manual['cv_std'],\n",
    "    fmt='k+',\n",
    "    ecolor='lightgray',\n",
    "    elinewidth=4,\n",
    "    capsize=0)\n",
    "plt.errorbar(\n",
    "    icon_modex_log10['ntrain'],\n",
    "    icon_modex_log10['hold-out-avg'],\n",
    "    icon_modex_log10['hold-out-std'],\n",
    "    fmt='ks',\n",
    "    ecolor='b',\n",
    "    elinewidth=4,\n",
    "    capsize=5)\n",
    "ax.grid()\n",
    "plt.ylabel('Avg ML Model Score of Training data set (R2)')\n",
    "plt.xlabel('Total number of samples in input data set')\n",
    "plt.legend(['Manual pre-ModEx iterations','ModEx iterations'])\n",
    "plt.savefig('ICON_ModEx_summary_simplified.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026cf5d-4da4-4d49-93b5-4b2fe53a1a1e",
   "metadata": {},
   "source": [
    "## Notes on usage of `scikit-learn`'s `roc_auc_score`:\n",
    "1. The observations (`y`) must be 0 or 1 (i.e. perfect binary classifiers).\n",
    "2. The predictions can be on a scale from 0 to 1 (i.e. confidence in each classification)\n",
    "3. In our case, we want to know how well hot spots and cold spots are classified. Ideally, we would not force a threshold (i.e. 500 mg O2/L/hour) - but if we have to force a classification based on the observations, we would then need to figure out some kind of normalization of the predictions to go between 0 and 1. The choice of normalization is also arbitary, so there's no way to avoid some kind of arbitrary thresholding in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95da73a-2bb9-4956-9ccc-a9167ceb92f3",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c2551-63e6-4b7c-b781-eafd039df2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete decompressed CONUS outline\n",
    "! rm -rf us.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f95f3-c568-4dec-8c14-6838b76cdf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
